{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56f4feeb-f28d-457e-ad9f-dd90b752977d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 316.9 MB 76 kB/s  eta 0:00:013   |███▎                            | 33.0 MB 119 kB/s eta 0:39:29     |█████▊                          | 57.1 MB 126 kB/s eta 0:34:07     |███████████▎                    | 111.5 MB 15.1 MB/s eta 0:00:14     |██████████████                  | 139.3 MB 15.5 MB/s eta 0:00:12     |███████████████▌                | 152.9 MB 10.6 MB/s eta 0:00:16     |███████████████████████▎        | 230.2 MB 4.1 MB/s eta 0:00:22     |█████████████████████████▎      | 250.6 MB 5.1 MB/s eta 0:00:14     |█████████████████████████▍      | 251.5 MB 5.1 MB/s eta 0:00:13\n",
      "\u001b[?25hCollecting py4j==0.10.9.7\n",
      "  Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "\u001b[K     |████████████████████████████████| 200 kB 16.9 MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425365 sha256=a1478bc8530c87e8665a573154de6f76712a201c9a4b623e21a8064bfb653dd5\n",
      "  Stored in directory: /Users/ankitrajvanshi/Library/Caches/pip/wheels/57/bd/14/ce9e21f2649298678d011fb8f71ed38ee70b42b94fef0be142\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.7 pyspark-3.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9cd414b9-0960-45b5-8c69-a9234752a95c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark[sql] in /Users/ankitrajvanshi/opt/anaconda3/lib/python3.9/site-packages (3.5.0)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /Users/ankitrajvanshi/opt/anaconda3/lib/python3.9/site-packages (from pyspark[sql]) (0.10.9.7)\n",
      "Collecting pyarrow>=4.0.0\n",
      "  Downloading pyarrow-14.0.1-cp39-cp39-macosx_10_14_x86_64.whl (26.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 26.9 MB 8.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.15 in /Users/ankitrajvanshi/opt/anaconda3/lib/python3.9/site-packages (from pyspark[sql]) (1.21.5)\n",
      "Requirement already satisfied: pandas>=1.0.5 in /Users/ankitrajvanshi/opt/anaconda3/lib/python3.9/site-packages (from pyspark[sql]) (1.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/ankitrajvanshi/opt/anaconda3/lib/python3.9/site-packages (from pandas>=1.0.5->pyspark[sql]) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/ankitrajvanshi/opt/anaconda3/lib/python3.9/site-packages (from pandas>=1.0.5->pyspark[sql]) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in /Users/ankitrajvanshi/opt/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas>=1.0.5->pyspark[sql]) (1.16.0)\n",
      "Installing collected packages: pyarrow\n",
      "Successfully installed pyarrow-14.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install 'pyspark[sql]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9cf469d-649a-4116-b154-298fc32bb2ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark[pandas_on_spark] in /Users/ankitrajvanshi/opt/anaconda3/lib/python3.9/site-packages (3.5.0)\n",
      "Requirement already satisfied: plotly in /Users/ankitrajvanshi/opt/anaconda3/lib/python3.9/site-packages (5.6.0)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /Users/ankitrajvanshi/opt/anaconda3/lib/python3.9/site-packages (from pyspark[pandas_on_spark]) (0.10.9.7)\n",
      "Requirement already satisfied: pyarrow>=4.0.0 in /Users/ankitrajvanshi/opt/anaconda3/lib/python3.9/site-packages (from pyspark[pandas_on_spark]) (14.0.1)\n",
      "Requirement already satisfied: numpy>=1.15 in /Users/ankitrajvanshi/opt/anaconda3/lib/python3.9/site-packages (from pyspark[pandas_on_spark]) (1.21.5)\n",
      "Requirement already satisfied: pandas>=1.0.5 in /Users/ankitrajvanshi/opt/anaconda3/lib/python3.9/site-packages (from pyspark[pandas_on_spark]) (1.4.2)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /Users/ankitrajvanshi/opt/anaconda3/lib/python3.9/site-packages (from plotly) (8.0.1)\n",
      "Requirement already satisfied: six in /Users/ankitrajvanshi/opt/anaconda3/lib/python3.9/site-packages (from plotly) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/ankitrajvanshi/opt/anaconda3/lib/python3.9/site-packages (from pandas>=1.0.5->pyspark[pandas_on_spark]) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/ankitrajvanshi/opt/anaconda3/lib/python3.9/site-packages (from pandas>=1.0.5->pyspark[pandas_on_spark]) (2021.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install 'pyspark[pandas_on_spark]' plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dcbe793f-cd6c-4c42-a8f2-dff4b5d7947b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark[connect] in /Users/ankitrajvanshi/opt/anaconda3/lib/python3.9/site-packages (3.5.0)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /Users/ankitrajvanshi/opt/anaconda3/lib/python3.9/site-packages (from pyspark[connect]) (0.10.9.7)\n",
      "Requirement already satisfied: numpy>=1.15 in /Users/ankitrajvanshi/opt/anaconda3/lib/python3.9/site-packages (from pyspark[connect]) (1.21.5)\n",
      "Collecting googleapis-common-protos>=1.56.4\n",
      "  Downloading googleapis_common_protos-1.61.0-py2.py3-none-any.whl (230 kB)\n",
      "\u001b[K     |████████████████████████████████| 230 kB 2.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pandas>=1.0.5 in /Users/ankitrajvanshi/opt/anaconda3/lib/python3.9/site-packages (from pyspark[connect]) (1.4.2)\n",
      "Collecting grpcio-status>=1.56.0\n",
      "  Downloading grpcio_status-1.59.3-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: pyarrow>=4.0.0 in /Users/ankitrajvanshi/opt/anaconda3/lib/python3.9/site-packages (from pyspark[connect]) (14.0.1)\n",
      "Collecting grpcio>=1.56.0\n",
      "  Downloading grpcio-1.59.3-cp39-cp39-macosx_10_10_universal2.whl (9.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 9.6 MB 25.3 MB/s eta 0:00:01     |█████████                       | 2.7 MB 25.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5\n",
      "  Downloading protobuf-4.25.1-cp37-abi3-macosx_10_9_universal2.whl (394 kB)\n",
      "\u001b[K     |████████████████████████████████| 394 kB 5.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /Users/ankitrajvanshi/opt/anaconda3/lib/python3.9/site-packages (from pandas>=1.0.5->pyspark[connect]) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/ankitrajvanshi/opt/anaconda3/lib/python3.9/site-packages (from pandas>=1.0.5->pyspark[connect]) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in /Users/ankitrajvanshi/opt/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas>=1.0.5->pyspark[connect]) (1.16.0)\n",
      "Installing collected packages: protobuf, grpcio, googleapis-common-protos, grpcio-status\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.19.1\n",
      "    Uninstalling protobuf-3.19.1:\n",
      "      Successfully uninstalled protobuf-3.19.1\n",
      "  Attempting uninstall: grpcio\n",
      "    Found existing installation: grpcio 1.42.0\n",
      "    Uninstalling grpcio-1.42.0:\n",
      "      Successfully uninstalled grpcio-1.42.0\n",
      "  Attempting uninstall: googleapis-common-protos\n",
      "    Found existing installation: googleapis-common-protos 1.53.0\n",
      "    Uninstalling googleapis-common-protos-1.53.0:\n",
      "      Successfully uninstalled googleapis-common-protos-1.53.0\n",
      "Successfully installed googleapis-common-protos-1.61.0 grpcio-1.59.3 grpcio-status-1.59.3 protobuf-4.25.1\n"
     ]
    }
   ],
   "source": [
    "!pip install 'pyspark[connect]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16be6110-326a-4abc-b176-243f44567397",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/11/20 00:20:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fccadc6f-6af6-40a3-9751-bbc4a4fed789",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[a: bigint, b: double, c: string, d: date, e: timestamp]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime, date\n",
    "import pandas as pd\n",
    "from pyspark.sql import Row\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "    Row(a=1, b=2., c='string1', d=date(2000, 1, 1), e=datetime(2000, 1, 1, 12, 0)),\n",
    "    Row(a=2, b=3., c='string2', d=date(2000, 2, 1), e=datetime(2000, 1, 2, 12, 0)),\n",
    "    Row(a=4, b=5., c='string3', d=date(2000, 3, 1), e=datetime(2000, 1, 3, 12, 0))\n",
    "])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2af67170-4d6a-4cfa-9f9a-bd80da648f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, regexp_replace, split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a3e8c3a6-b55a-487c-bbf0-df3ea23d68b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"movies.dat\", sep='::', schema='MovieID int, oldTitle string, combinedGenres string')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6944756f-f3ac-43e8-a817-ad47b1ddfc9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+----+--------------------+\n",
      "|MovieID|               Title|Year|              Genres|\n",
      "+-------+--------------------+----+--------------------+\n",
      "|      1|          Toy Story |1995|[Animation, Child...|\n",
      "|      2|            Jumanji |1995|[Adventure, Child...|\n",
      "|      3|   Grumpier Old Men |1995|   [Comedy, Romance]|\n",
      "|      4|  Waiting to Exhale |1995|     [Comedy, Drama]|\n",
      "|      5|Father of the Bri...|1995|            [Comedy]|\n",
      "|      6|               Heat |1995|[Action, Crime, T...|\n",
      "|      7|            Sabrina |1995|   [Comedy, Romance]|\n",
      "|      8|       Tom and Huck |1995|[Adventure, Child...|\n",
      "|      9|       Sudden Death |1995|            [Action]|\n",
      "|     10|          GoldenEye |1995|[Action, Adventur...|\n",
      "|     11|American Presiden...|1995|[Comedy, Drama, R...|\n",
      "|     12|Dracula: Dead and...|1995|    [Comedy, Horror]|\n",
      "|     13|              Balto |1995|[Animation, Child...|\n",
      "|     14|              Nixon |1995|             [Drama]|\n",
      "|     15|   Cutthroat Island |1995|[Action, Adventur...|\n",
      "|     16|             Casino |1995|   [Drama, Thriller]|\n",
      "|     17|Sense and Sensibi...|1995|    [Drama, Romance]|\n",
      "|     18|         Four Rooms |1995|          [Thriller]|\n",
      "|     19|Ace Ventura: When...|1995|            [Comedy]|\n",
      "|     20|        Money Train |1995|            [Action]|\n",
      "+-------+--------------------+----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Split title_year column into title, year\n",
    "df_modified = df.withColumn(\"oldTitle_Modified\", regexp_replace(\"oldTitle\", \"[)]\", \"(\"))\n",
    "split_df = df_modified.withColumn(\"Title_Year\", split(\"oldTitle_Modified\", '\\\\('))\n",
    "final_df = split_df.withColumn(\"Title\", col(\"Title_Year\")[0]).withColumn(\"Year\", col(\"Title_Year\")[1]).withColumn(\"Genres\", split(\"combinedGenres\", '\\\\|'))\n",
    "final_df = final_df.drop(\"oldTitle\", \"oldTitle_Modified\", \"Title_Year\", \"combinedGenres\")\n",
    "final_df.show()\n",
    "final_df.write.json(\"movies.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5daae5c4-0bd0-494c-b072-d3d6c5fa4820",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 28:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+---------+\n",
      "|UserID|MovieID|Rating|Timestamp|\n",
      "+------+-------+------+---------+\n",
      "|     1|   1193|     5|978300760|\n",
      "|     1|    661|     3|978302109|\n",
      "|     1|    914|     3|978301968|\n",
      "|     1|   3408|     4|978300275|\n",
      "|     1|   2355|     5|978824291|\n",
      "|     1|   1197|     3|978302268|\n",
      "|     1|   1287|     5|978302039|\n",
      "|     1|   2804|     5|978300719|\n",
      "|     1|    594|     4|978302268|\n",
      "|     1|    919|     4|978301368|\n",
      "|     1|    595|     5|978824268|\n",
      "|     1|    938|     4|978301752|\n",
      "|     1|   2398|     4|978302281|\n",
      "|     1|   2918|     4|978302124|\n",
      "|     1|   1035|     5|978301753|\n",
      "|     1|   2791|     4|978302188|\n",
      "|     1|   2687|     3|978824268|\n",
      "|     1|   2018|     4|978301777|\n",
      "|     1|   3105|     5|978301713|\n",
      "|     1|   2797|     4|978302039|\n",
      "+------+-------+------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('ratings.dat', sep='::', schema='UserID int, MovieID int, Rating int, Timestamp long')\n",
    "df_single_partition = df.coalesce(1)\n",
    "df_single_partition.write.csv(\"ratings.csv\", header=True, mode=\"overwrite\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ff403abd-af71-4745-9937-0f11f22069a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+---+----------+-------+\n",
      "|UserID|Gender|Age|Occupation|Zipcode|\n",
      "+------+------+---+----------+-------+\n",
      "|     1|     F|  1|        10|  48067|\n",
      "|     2|     M| 56|        16|  70072|\n",
      "|     3|     M| 25|        15|  55117|\n",
      "|     4|     M| 45|         7|   2460|\n",
      "|     5|     M| 25|        20|  55455|\n",
      "|     6|     F| 50|         9|  55117|\n",
      "|     7|     M| 35|         1|   6810|\n",
      "|     8|     M| 25|        12|  11413|\n",
      "|     9|     M| 25|        17|  61614|\n",
      "|    10|     F| 35|         1|  95370|\n",
      "|    11|     F| 25|         1|   4093|\n",
      "|    12|     M| 25|        12|  32793|\n",
      "|    13|     M| 45|         1|  93304|\n",
      "|    14|     M| 35|         0|  60126|\n",
      "|    15|     M| 25|         7|  22903|\n",
      "|    16|     F| 35|         0|  20670|\n",
      "|    17|     M| 50|         1|  95350|\n",
      "|    18|     F| 18|         3|  95825|\n",
      "|    19|     M|  1|        10|  48073|\n",
      "|    20|     M| 25|        14|  55113|\n",
      "+------+------+---+----------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('users.dat', sep='::', schema='UserID int, Gender string, Age int, Occupation int, Zipcode int')\n",
    "\n",
    "df.show()\n",
    "df.write.json(\"users.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b527130-5776-4956-aec7-037f365b3989",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
